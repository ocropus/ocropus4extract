{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine, LTChar, LTPage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_characters_into_words(characters, threshold=1.5, toffset=1):\n",
    "    def median(lst):\n",
    "        sorted_lst = sorted(lst)\n",
    "        mid = len(lst) // 2\n",
    "        return (sorted_lst[mid] + sorted_lst[-mid-1]) / 2\n",
    "\n",
    "    words = []\n",
    "    current_word = []\n",
    "\n",
    "    if not characters:\n",
    "        return words\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(characters) - 1):\n",
    "        x1_current, _, x2_current, _ = characters[i]['bbox']\n",
    "        x1_next, _, _, _ = characters[i + 1]['bbox']\n",
    "        distances.append(x1_next - x2_current)\n",
    "\n",
    "    threshold = median(distances) * threshold + toffset\n",
    "\n",
    "    for i in range(len(characters) - 1):\n",
    "        current_word.append(characters[i])\n",
    "\n",
    "        x1_current, _, x2_current, _ = characters[i]['bbox']\n",
    "        x1_next, _, _, _ = characters[i + 1]['bbox']\n",
    "\n",
    "        if x1_next - x2_current > threshold:\n",
    "            words.append(current_word)\n",
    "            current_word = []\n",
    "\n",
    "    current_word.append(characters[-1])\n",
    "    words.append(current_word)\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_threshold(characters):\n",
    "    def median(lst):\n",
    "        assert len(lst) > 0\n",
    "        if len(lst) == 1:\n",
    "            return lst[0]\n",
    "        sorted_lst = sorted(lst)\n",
    "        mid = len(lst) // 2\n",
    "        return (sorted_lst[mid] + sorted_lst[-mid-1]) / 2\n",
    "\n",
    "    if not characters:\n",
    "        return 0\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(characters) - 1):\n",
    "        x1_current, _, x2_current, _ = characters[i]['bbox']\n",
    "        x1_next, _, _, _ = characters[i + 1]['bbox']\n",
    "        distances.append(x1_next - x2_current)\n",
    "\n",
    "    if len(distances) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return median(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def create_word_dict(characters: List[Dict[str, str]]) -> Dict[str, str]:\n",
    "    x_min = min(char['bbox'][0] for char in characters)\n",
    "    y_min = min(char['bbox'][1] for char in characters)\n",
    "    x_max = max(char['bbox'][2] for char in characters)\n",
    "    y_max = max(char['bbox'][3] for char in characters)\n",
    "    \n",
    "    word_bbox = (x_min, y_min, x_max, y_max)\n",
    "    word_text = ''.join([c['text'] for c in characters])\n",
    "    return {'word': word_text, 'bbox': word_bbox}\n",
    "\n",
    "def group_characters_into_words(characters: List[Dict[str, str]], threshold: float) -> List[Dict[str, str]]:\n",
    "    words = []\n",
    "    current_word = []\n",
    "\n",
    "    # Return an empty list if there are no characters\n",
    "    if not characters:\n",
    "        return words\n",
    "\n",
    "    # Iterate through all characters, except the last one\n",
    "    for i in range(len(characters) - 1):\n",
    "        # Add the current character to the current_word list\n",
    "        current_word.append(characters[i])\n",
    "\n",
    "        # Get the current and next character's x-coordinates\n",
    "        x1_current, _, x2_current, _ = characters[i]['bbox']\n",
    "        x1_next, _, _, _ = characters[i + 1]['bbox']\n",
    "\n",
    "        # Check if the distance between the current and next character is greater than the threshold\n",
    "        if x1_next < x2_current-10 or x1_next - x2_current > threshold:\n",
    "            # If so, create a word from the current_word list and append it to the words list\n",
    "            words.append(create_word_dict(current_word))\n",
    "            # Reset the current_word list\n",
    "            current_word = []\n",
    "\n",
    "    # Add the last character to the current_word list\n",
    "    # This is necessary because the loop above processes pairs of consecutive characters,\n",
    "    # so the last character is not yet included in any word\n",
    "    current_word.append(characters[-1])\n",
    "\n",
    "    # Create a word from the remaining characters in the current_word list and append it to the words list\n",
    "    words.append(create_word_dict(current_word))\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dictionary = set()\n",
    "\n",
    "def read_system_dictionary():\n",
    "    global global_dictionary\n",
    "    with open('/usr/share/dict/words', 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip().lower()\n",
    "            global_dictionary.add(word)\n",
    "\n",
    "read_system_dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_word_in_dictionary(word):\n",
    "    global global_dictionary\n",
    "    cleaned_word = re.sub(r'\\W+', '', word).lower()\n",
    "    return cleaned_word in global_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_grouping(characters, predicate, thresholds):\n",
    "    max_count = 0\n",
    "    optimal_words = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        words = group_characters_into_words(characters, threshold)\n",
    "        count = sum(1 for word_info in words if predicate(word_info['word']))\n",
    "        print(\">>>\", threshold, count)\n",
    "\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            optimal_words = words\n",
    "\n",
    "    return optimal_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def estimate_and_group_words(characters: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    candidates = []\n",
    "    for i in range(1, 10):\n",
    "        words = group_characters_into_words(characters, i)\n",
    "        quality = sum(1 for word_info in words if is_word_in_dictionary(word_info['word']))\n",
    "        candidates.append((quality, words))\n",
    "    candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "    return candidates[0][1]\n",
    "\n",
    "def extract_word_bounding_boxes(pdf_path: str) -> List[Dict[str, object]]:\n",
    "    # Type check for pdf_path\n",
    "    assert isinstance(pdf_path, str), f\"pdf_path should be a string, got {type(pdf_path)} instead.\"\n",
    "\n",
    "    pages_bounding_boxes = []\n",
    "\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        parser = PDFParser(file)\n",
    "        document = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # Process each page in the PDF\n",
    "        pageno = 0\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            pageno += 1\n",
    "            print(\"page\", pageno)\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "            page_bbox = None\n",
    "\n",
    "            # Get the bounding box for the entire page\n",
    "            if isinstance(layout, LTPage):\n",
    "                page_bbox = layout.bbox\n",
    "\n",
    "            word_bounding_boxes = []\n",
    "\n",
    "            # Iterate through the layout elements\n",
    "            for element in layout:\n",
    "                # Process text boxes\n",
    "                char_bounding_boxes = []\n",
    "                if isinstance(element, LTTextBox):\n",
    "                    # Process text lines within the text box\n",
    "                    for text_line in element:\n",
    "                        if isinstance(text_line, LTTextLine):\n",
    "                            \n",
    "                            # Extract character bounding boxes from the text line\n",
    "                            for character in text_line:\n",
    "                                if isinstance(character, LTChar):\n",
    "                                    char_bounding_boxes.append({\n",
    "                                        'text': character.get_text(),\n",
    "                                        'bbox': character.bbox\n",
    "                                    })\n",
    "\n",
    "                # Estimate and group words for the given text line\n",
    "                # words = estimate_and_group_words(char_bounding_boxes)\n",
    "                words = estimate_and_group_words(char_bounding_boxes)\n",
    "\n",
    "                # Append the words and their bounding boxes to the word_bounding_boxes list\n",
    "                word_bounding_boxes.extend(words)\n",
    "\n",
    "                if True:\n",
    "                    text = \" \".join([w[\"word\"] for w in words])\n",
    "                    print(\"---\")\n",
    "                    print(textwrap.fill(text, 80))\n",
    "\n",
    "            # Add the page bounding box and word bounding boxes to the output list\n",
    "            pages_bounding_boxes.append({\n",
    "                'page_bbox': page_bbox,\n",
    "                'word_bboxes': word_bounding_boxes\n",
    "            })\n",
    "\n",
    "    return pages_bounding_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_words(words):\n",
    "    text = \" \".join([w[\"word\"] for w in words])\n",
    "    print(\"---\")\n",
    "    print(textwrap.fill(text, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from pdfminer.layout import LTFigure\n",
    "\n",
    "def extract_image_bounding_boxes(page: LTPage) -> List[Dict[str, tuple]]:\n",
    "    image_bounding_boxes = []\n",
    "\n",
    "    # Iterate through the layout elements\n",
    "    for element in page:\n",
    "        # Process image elements\n",
    "        if isinstance(element, LTFigure):\n",
    "            image_bbox = element.bbox\n",
    "\n",
    "            # Append the image bounding box to the image_bounding_boxes list\n",
    "            image_bounding_boxes.append({'bbox': image_bbox})\n",
    "\n",
    "    return image_bounding_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def process_page(page: LTPage) -> Dict[str, object]:\n",
    "    page_bbox = page.bbox\n",
    "    word_bounding_boxes = []\n",
    "\n",
    "    # Iterate through the layout elements\n",
    "    for element in page:\n",
    "        # Process text boxes\n",
    "        if isinstance(element, LTTextBox):\n",
    "            char_bounding_boxes = []\n",
    "\n",
    "            # Process text lines within the text box\n",
    "            for text_line in element:\n",
    "                if isinstance(text_line, LTTextLine):\n",
    "\n",
    "                    # Extract character bounding boxes from the text line\n",
    "                    for character in text_line:\n",
    "                        if isinstance(character, LTChar):\n",
    "                            char_bounding_boxes.append({\n",
    "                                'text': character.get_text(),\n",
    "                                'bbox': character.bbox\n",
    "                            })\n",
    "\n",
    "            # Estimate and group words for the given text line\n",
    "            words = estimate_and_group_words(char_bounding_boxes)\n",
    "            print_words(words)\n",
    "\n",
    "            # Append the words and their bounding boxes to the word_bounding_boxes list\n",
    "            word_bounding_boxes.extend(words)\n",
    "\n",
    "    return {'page_bbox': page_bbox, 'word_bboxes': word_bounding_boxes}\n",
    "\n",
    "def extract_word_bounding_boxes(pdf_path: str) -> List[Dict[str, object]]:\n",
    "    # Type check for pdf_path\n",
    "    assert isinstance(pdf_path, str), f\"pdf_path should be a string, got {type(pdf_path)} instead.\"\n",
    "\n",
    "    pages_bounding_boxes = []\n",
    "\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        parser = PDFParser(file)\n",
    "        document = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # Process each page in the PDF\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "\n",
    "            # Get the page and word bounding boxes\n",
    "            page_data = process_page(layout)\n",
    "            page_data[\"image_bboxes\"] = extract_image_bounding_boxes(layout)\n",
    "\n",
    "            # Add the page bounding box and word bounding boxes to the output list\n",
    "            pages_bounding_boxes.append(page_data)\n",
    "\n",
    "    return pages_bounding_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "6 1 0 2\n",
      "---\n",
      "p e S 2\n",
      "---\n",
      "]\n",
      "---\n",
      "V C . s c [\n",
      "---\n",
      "2 v 1 7 9 5 0 . 8 0 6 1 : v i X r a\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "Mohsen Fayyaz1, Mohammad Hajizadeh Saﬀar1, Mohammad Sabokrou1, Mahmood Fathy2,\n",
      "Reinhard Klette3, Fay Huang4\n",
      "---\n",
      "1Malek-Ashtar University of Technology, 2 Iran University of Science and\n",
      "Technology, 3Auckland University of Technology, 3National Ilan University\n",
      "---\n",
      "Abstract. This paper presents a novel method to involve both spa- tial and\n",
      "temporal features for semantic segmentation of street scenes. Current work on\n",
      "convolutional neural networks (CNNs) has shown that CNNs provide advanced\n",
      "spatial features supporting a very good perfor- mance of solutions for the\n",
      "semantic segmentation task. We investigate how involving temporal features also\n",
      "has a good eﬀect on segmenting video data. We propose a module based on a long\n",
      "short-term memory (LSTM) architecture of a recurrent neural network for\n",
      "interpreting the temporal characteristics of video frames over time. Our system\n",
      "takes as input frames of a video and produces a correspondingly-sized output;\n",
      "for segmenting the video our method combines the use of three compo- nents:\n",
      "First, the regional spatial features of frames are extracted using a CNN; then,\n",
      "using LSTM the temporal features are added; ﬁnally, by deconvolving the spatio-\n",
      "temporal features we produce pixel-wise predic- tions. Our key insight is to\n",
      "build spatio-temporal convolutional networks (spatio-temporal CNNs) that have an\n",
      "end-to-end architecture for seman- tic video segmentation. We adapted fully some\n",
      "known convolutional net- work architectures (such as FCN-AlexNet and FCN-VGG16),\n",
      "and dilated convolution into our spatio-temporal CNNs. Our spatio-temporal CNNs\n",
      "achieve state-of-the-art semantic segmentation, as demonstrated for the Camvid\n",
      "and NYUDv2 datasets.\n",
      "---\n",
      "1\n",
      "---\n",
      "Introduction\n",
      "---\n",
      "Semantic segmentation of video data is a fundamental task for scene understand-\n",
      "ing. For many computer vision applications, semantic segmentation is considered\n",
      "as being (just) a pre-processing task. Consequently, the performance of seman-\n",
      "tic segmentation has a direct eﬀect on subsequent computer vision solutions\n",
      "which depend on it. Self-driving cars is one of the areas in technology that has\n",
      "received much attention recently. These cars can detect surroundings using ad-\n",
      "vanced driver assistance systems (ADAS) that consist of many diﬀerent systems\n",
      "such as radar, GPS, computer vision, and in-car networking to bring safety to\n",
      "driving and roads. One of the main processes for the computer vision part of\n",
      "these systems can be identiﬁed as being semantic segmentation of all objects in\n",
      "surroundings to transmit accurate and complete information to the ADAS system\n",
      "such that the system can make the best decision to avoid accidents.\n",
      "---\n",
      "2\n",
      "---\n",
      "Authors Suppressed Due to Excessive Length\n",
      "---\n",
      "Segmentation is typically approached as a classiﬁcation problem. First, us- ing\n",
      "a set of labeled video frames, the characteristics of all segments (classes) are\n",
      "learned. These characteristics are used for labeling the pixels of test frames\n",
      "[1,51]. Recently, deep learning methods, especially CNNs, ensured state-of-the-\n",
      "art per- formance in diﬀerent areas of computer vision, such as in image\n",
      "classiﬁcation [26], object detection [14], or activity recognition [40].\n",
      "---\n",
      "We consider the application of advanced features, extracted by using CNNs, for\n",
      "semantic video segmentation. Semantic segmentation methods use both given image\n",
      "data at selected locations as well as a semantic context. A set of pixels is\n",
      "usually predicted as deﬁning one class (or even one segment) if connected, and\n",
      "also referring to one particular semantic interpretation.\n",
      "---\n",
      "Previous methods for video segmentation have eﬃciently exploited CNNs, but they\n",
      "did not use temporal features; of course, temporal features can be useful for\n",
      "interpreting a video semantically. For example, the authors of [1,51]\n",
      "represented and interpreted video frames using a deep learning method, but the\n",
      "main disadvantage of their methods is that they consider those frames as being\n",
      "independent from each other. Neglecting the time dimension in video data\n",
      "basically means that the given raw data are down-sampled without using fully\n",
      "given information. Using temporal features can help the system to\n",
      "distinguishing, for example, between two objects of diﬀerent classes having the\n",
      "same spatial features but showing diﬀerences in the time feature dimension.\n",
      "---\n",
      "Consequently, we propose a method which uses a similar paradigm for ex- tracting\n",
      "spatial features (as in the cited papers), but which diﬀers by also using\n",
      "temporal features (i.e. features of a continues sequences of frames). We propose\n",
      "to identify components which can be embedded “on top” of spatially extracted\n",
      "features maps in individual frames. Such a component can be seen as being\n",
      "equipped with a set of memory cells which save the assigned regions in previ-\n",
      "ous frames. This allows us that relations between regions, available in previous\n",
      "frames, can be used to deﬁne temporal features. We process the current video\n",
      "frame by using the spatio-temporal output features of our processing modules.\n",
      "Similar to other segmentation methods, we use then some fully convolutional\n",
      "layers to perform regional semantic classiﬁcation. In our method, these fully\n",
      "convolutional layers perform spatio-temporal classiﬁcations. Finally, we use a\n",
      "deconvolution procedure for mapping (i.e. scaling) the obtained predictions into\n",
      "the original carrier (i.e. the image grid) of the given frames for having a\n",
      "pixel-wise prediction. See Fig. 1.\n",
      "---\n",
      "CNN-based methods usually combine two components, where one is for de- scribing\n",
      "and inferring a class of diﬀerent regions of a video frame as a feature map, and\n",
      "another one for performing an up-sampling of the labeled feature maps to the\n",
      "size of the given video frames. An advantage of our method is that we can adjust\n",
      "and embed our proposed module into the end of the ﬁrst component (before\n",
      "inferring the labels) of current CNN-based methods as an end-to-end network. We\n",
      "show that the proposed changes in the network lead to an improve- ment in the\n",
      "performance of state-of-the-art methods, such as, FCN-8 [32] and dilated\n",
      "convolution [51].\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "3\n",
      "---\n",
      "Fig. 1. A spatio-temporal fully convolutional Alexnet architecture, later also\n",
      "to be discussed in Section 4.1\n",
      "---\n",
      "The main contributions of this paper are as follows:\n",
      "---\n",
      "– The proposed method can be easily adapted for enhancing already published\n",
      "---\n",
      "state-of-the art methods for improving their performance.\n",
      "---\n",
      "– We propose an end-to-end network for semantic video segmentation in re-\n",
      "---\n",
      "spect to both spatial and temporal features.\n",
      "---\n",
      "– We propose a module for transforming traditional, fully convolutional net-\n",
      "---\n",
      "works into spatio-temporal CNNs.\n",
      "---\n",
      "– We outperformed state-of-the art methods on two standard benchmarks.\n",
      "---\n",
      "The rest of this paper is organized as follows. Top-ranked related work on se-\n",
      "mantic video segmentation is reviewed in Section 2. Section 3 introduces the\n",
      "proposed method. The performance of our method is shown in Section 4. Sec- tion\n",
      "5 concludes the paper.\n",
      "---\n",
      "2 Related Work\n",
      "---\n",
      "There is a wide range of approaches that have been published so far for video\n",
      "segmentation. Some of them have advantages over others. These approaches can be\n",
      "categorized based on the kind of data that they operate on, the method that is\n",
      "used to classify the segments, and the kind of segmentation that they can\n",
      "produce.\n",
      "---\n",
      "Some approaches focus on binary classes such as foreground and background\n",
      "segmentation [2,4]. This ﬁeld includes also some work that has a focus on\n",
      "anomaly detection [37] since authors use a single-class classiﬁcation scheme and\n",
      "con- structed an outlier detection method for all other categories. Some other\n",
      "ap- proaches concentrate on multi-class segmentation [6,29,30,45].\n",
      "---\n",
      "Recently created video datasets provide typically image data in RGB format.\n",
      "Correspondingly, there is no recent research on gray-scale semantic video seg-\n",
      "---\n",
      "4\n",
      "---\n",
      "Authors Suppressed Due to Excessive Length\n",
      "---\n",
      "mentation; the use of RGB data is common standard, see [12,23,29,30,48]. There\n",
      "are also some segmentation approaches that use RGB-D datasets [17,18,33].\n",
      "---\n",
      "Feature selection is a challenging step in every machine learning approach. The\n",
      "system’s accuracy is very much related to the set of features that are chosen\n",
      "for learning and model creation. Diﬀerent methods have been proposed for the\n",
      "segmentation-related feature extraction phase.\n",
      "---\n",
      "2.1 Feature Extraction\n",
      "---\n",
      "We recall brieﬂy some common local or global feature extraction methods in the\n",
      "semantic segmentation ﬁeld. These feature extraction methods are commonly used\n",
      "after having super-voxels extracted from video frames [30].\n",
      "---\n",
      "Pixel color features are features used in almost every semantic segmentation\n",
      "system [12,23,29,30,33]. Those includes three channel values for RGB or HSV im-\n",
      "ages, and also values obtained by histogram equalization methods. The histogram\n",
      "of oriented gradients (HOG) deﬁnes a set of features combining at sets of pixels\n",
      "approximated gradient values for partial derivatives in x or y direction[23,29].\n",
      "Some approaches also used other histogram deﬁnitions such as the hue color\n",
      "histogram or a texton histogram [48].\n",
      "---\n",
      "Further appearance-based features are deﬁned as across-boundary appear-\n",
      "---\n",
      "ance features, texture features, or spatio-temporal appearance features; see\n",
      "[12,23,29,30]. Some approaches that use RGB-D datasets, also include\n",
      "3-dimensional (3D) po- sitions or 3D optical ﬂow features [18,33]. Recently,\n",
      "some approaches are pub- lished that use CNNs for feature extraction; using pre-\n",
      "trained models for feature representation is common in [1,17,49].\n",
      "---\n",
      "After collecting a set of features for learning, a model must be chosen for\n",
      "training a classiﬁer for segmentation. Several methods have been provided al-\n",
      "ready for this purpose, and we recall a few.\n",
      "---\n",
      "2.2 Segmentation Methods\n",
      "---\n",
      "Some researches wanted to propose a (very) general image segmentation ap-\n",
      "proach. For this reason, they concentrated on using unsupervised segmentation.\n",
      "This ﬁeld includes clustering algorithms such as k-means and mean-shift [31], or\n",
      "graph-based algorithms [12,18,23,47].\n",
      "---\n",
      "A random decision forest (RDF) can be used for deﬁning another segmenta- tion\n",
      "method that is a kind of a classiﬁer composed of multiple classiﬁers which are\n",
      "trained and enhanced by using randomness extensively [16,36]. The support vector\n",
      "machine (SVM) [43] or a Markov random ﬁeld (MRF) [38,46] are further methods\n",
      "used for segmentation but not as popular as the conditional random ﬁeld (CRF)\n",
      "that is in widespread use in recent work[5,29,35].\n",
      "---\n",
      "Neural networks are a very popular method for image segmentation, espe- cially\n",
      "with the recent success of using convolutional neural network in the se- mantic\n",
      "segmentation ﬁeld. Like for many other vision tasks, neural networks have become\n",
      "very useful [1,14,17,20,32,49].\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "5\n",
      "---\n",
      "Fully convolutional networks (FCNs) are one of the topics that interest re-\n",
      "searchers recently. An FCN is based on the idea of extending a convolutional\n",
      "network (ConvNet) for arbitrary-sized inputs [32]. On the way of its develop-\n",
      "ment, it has been used for 1-dimensional (1D) and 2-dimensional (2D) inputs\n",
      "[34,44], and for solving various tasks such as image restoration, sliding window\n",
      "detection, depth estimation, boundary prediction, or semantic segmentation. In\n",
      "recent years, many approaches use ConvNets as feature extractor [1,17,49]. Some\n",
      "approaches turn ConvNets into FCNs by discarding the ﬁnal classiﬁer layer, and\n",
      "convert all fully connected layers into convolutions. By this change, authors\n",
      "use a front-end module for solving their vision tasks [1,14,17,20,32,49].\n",
      "---\n",
      "Recently, a new convolutional network module has been introduced by Yu and\n",
      "Fisher [51] that is especially designed for dense prediction. It uses dilated\n",
      "convolutions for multi-scale contextual information aggregation, and achieves\n",
      "some enhancements in semantic segmentation compared to previous methods. Kundu\n",
      "and Abhijit [27] optimized the mapping of pixels into a Euclidean feature space;\n",
      "they achieve even better results for semantic segmentation than [51] by using a\n",
      "graphical CRF model.\n",
      "---\n",
      "Many approaches that have been introduced in this ﬁeld have not yet used\n",
      "temporal features, especially in the ﬁeld of deep CNNs [12,18,23,29,30,48].\n",
      "These approaches cannot be identiﬁed as being end-to-end methods, which points\n",
      "to an essential disadvantage when applying these approaches. Some approaches use\n",
      "deep CNNs [17,27] by introducing an end-to-end architecture for also us- ing\n",
      "spatio-temporal features for semantic labeling. However, none of them can change\n",
      "the size of time windows dynamically.\n",
      "---\n",
      "Long short-term memory (LSTM) is a memory cell module that was intro- duced by\n",
      "[13,19]. It has many advantages such as the ability to support very large time\n",
      "windows, the ability to change time windows dynamically, the ability to handle\n",
      "noise, distributed representations, continuous values, and so forth. We propose\n",
      "for the ﬁrst time an approach that uses a deep CNN network with LSTM modules as\n",
      "an end-to-end trainable architecture for semantic video segmentation and\n",
      "labeling.\n",
      "---\n",
      "3 The Proposed Method\n",
      "---\n",
      "3.1 Overall scheme\n",
      "---\n",
      "We have four key steps in our method as shown in Fig. 2. We feed the frame It\n",
      "(i.e. the tth frame of a video), into a FCN network. This network down-samples\n",
      "the input images and describes a frame It, deﬁned on an image grid Ω of size W ×\n",
      "H, as a features set S1..m in m diﬀerent maps. The input is It and the output of\n",
      "the latest layer (i.e. of lowest resolution) of the FCN is S1..m of size W\n",
      "(cid:48) × H(cid:48), where W (cid:48) (cid:28) W and H(cid:48) (cid:28) H. As a\n",
      "result, frame It is represented as a feature set {S1..m}. Every point (i, j),\n",
      "with 1 ≤ i ≤ W (cid:48) and 1 ≤ j ≤ H(cid:48), in S1..m t\n",
      "---\n",
      "is a descriptor of size m for a region (receptive ﬁeld) in It.\n",
      "---\n",
      "t\n",
      "---\n",
      "t\n",
      "---\n",
      "t\n",
      "---\n",
      "We put our spatio-temporal module on top of the ﬁnal convolutional layer. } will\n",
      "be represented as a spatio-temporal feature set of\n",
      "---\n",
      "So, feature set {S1..m\n",
      "---\n",
      "t\n",
      "---\n",
      "6\n",
      "---\n",
      "Authors Suppressed Due to Excessive Length\n",
      "---\n",
      "Fig. 2. Overall scheme for our proposed end-to-end network architecture. The\n",
      "LSTMs are used for inferring the relations between spatial features which are\n",
      "extracted from the frames of the video\n",
      "---\n",
      "}(i,j) by our spatio-temporal module. By applying an FCN classiﬁer layer {ST\n",
      "1..m t on top of these features, we predict the semantic classes of these\n",
      "regions in the video. Finally, we up-sample these predictions to the size of the\n",
      "It frame. In following subsections, the methodologies that have been used in\n",
      "this approach, will be described.\n",
      "---\n",
      "3.2 Fully Convolutional Network\n",
      "---\n",
      "Convolutional neural networks (CNNs) are applied for a large set of vision\n",
      "tasks. Some researchers improve CNNs by changing its basic architecture and\n",
      "intro- ducing new architectures. Recently, fully convolutional networks (FCNs)\n",
      "have been introduced by discarding the ﬁnal classiﬁer layer, and by converting\n",
      "all fully connected layers into convolutional layers. We follow this principle.\n",
      "---\n",
      "3.3 LSTM\n",
      "---\n",
      "A long short-term memory (LSTM) network is a special kind of recurrent neu- ral\n",
      "networks (RNNs) that have been introduced by [19] to solve the vanishing\n",
      "gradient problem and for remembering information over long periods. For an\n",
      "example of a basic RNN and an LSTM cell, see Fig. 3. LSTMs are not conﬁned to\n",
      "ﬁxed-length inputs or outputs, and this advantage makes them powerful for\n",
      "solving sequential problems.\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "7\n",
      "---\n",
      "Fig. 3. An example of a basic LSTM cell (left) and a basic RNN cell (right).\n",
      "Figure follows a drawing in [9]\n",
      "---\n",
      "Each LSTM module consists of a memory cell and a number of input and output\n",
      "gates that control the information ﬂow in a sequence and prevent it from loosing\n",
      "important information in a time series. Assuming St as the input of an LSTM\n",
      "module at time t, the cell activation is as formulated in the following\n",
      "equations:\n",
      "---\n",
      "it = σ(Wxixt + Whiht−1 + bi) ft = σ(Wxf xt + Whf ht−1 + bf ) ot = σ(Wxoxt +\n",
      "Whoht−1 + bo) gt = φ(Wxcxt + Whcht−1 + bc) ct = ft (cid:12) ct−1 + it (cid:12)\n",
      "gt ht = ot (cid:12) φ(ct)\n",
      "---\n",
      "(1)\n",
      "---\n",
      "(2)\n",
      "---\n",
      "(3)\n",
      "---\n",
      "(4)\n",
      "---\n",
      "(5)\n",
      "---\n",
      "(6)\n",
      "---\n",
      "where σ and φ are symbols for a sigmoid and the tanh function, respectively.\n",
      "Symbol ht ∈ RN denotes a hidden state with N units, and ct ∈ RN is the memory\n",
      "cell. By it ∈ RN , ft ∈ RN , ot ∈ RN , and gt ∈ RN we denote the input gate,\n",
      "forget gate, output gate, and input modulation gate at time t, respectively.\n",
      "Symbol (cid:12) stands for element-wise multiplication.\n",
      "---\n",
      "3.4 Spatio-Temporal Module In regards to every W (cid:48) × H(cid:48) region of\n",
      "It, which is described by an FCN as an Ω grid, an LSTM is embedded (see Section\n",
      "3.1). Thus we have altogether W (cid:48) × H(cid:48) LSTMs. Element {S1..m}(i,j)\n",
      "deﬁnes a spatial characteristics of a region in the It frame. These\n",
      "characteristics are given to LSTM(i,j) for processing; it infers a relation with\n",
      "spatial features of equivalent regions in frames previous to frame It. With this\n",
      "“trick”, both spatial and temporal features of a frame are considered. (Note\n",
      "that LSTM(i,j)({S1..m }(i,j)) where S and ST are spatial and spatio-temporal\n",
      "features, respectively).\n",
      "---\n",
      "}(i,j)) = {ST 1..m\n",
      "---\n",
      "t\n",
      "---\n",
      "t\n",
      "---\n",
      "t\n",
      "---\n",
      "We embed one LSTM for each region. Equation (7) shows a representation\n",
      "---\n",
      "of frame It with respect to our suggested spatial and temporal features:\n",
      "---\n",
      "Ω(cid:48)\n",
      "---\n",
      "t(i, j) = (LST M (i,j)(Ωt(i, j))\n",
      "---\n",
      "(7)\n",
      "---\n",
      "8\n",
      "---\n",
      "Authors Suppressed Due to Excessive Length\n",
      "---\n",
      "where the size of Ω(cid:48) is equal to that of Ω, and value m speciﬁes a map\n",
      "which assigns spatio-temporal features to every point for describing an\n",
      "equivalent re- gion (i.e. a segment) in It. Now, similar to other methods\n",
      "[1,32,51], the labels for points in Ω(cid:48) are predicted and up-sampled to\n",
      "the frame at the original size. The overall update function can be brieﬂy\n",
      "speciﬁed as follows:\n",
      "---\n",
      "Ω(cid:48)\n",
      "---\n",
      "t(i, j) = σ(Wxoxt + Whoht−1 + bo) (cid:12) φ(ft (cid:12) ct−1 + it (cid:12) gt)\n",
      "---\n",
      "(8)\n",
      "---\n",
      "Altogether, we introduced an operator layer to several LSTMs for properly rep-\n",
      "resenting the temporal features.\n",
      "---\n",
      "This proposed network executes and processes the input frames as an end-\n",
      "---\n",
      "to-end network. Figure 2 shows the overall scheme of our method.\n",
      "---\n",
      "3.5 Deconvolution\n",
      "---\n",
      "Interpolation is a common method for mapping outputs into dense pixels. There\n",
      "are several interpolation (or upsampling) algorithms such as bilinear, non-\n",
      "linear, cubic, and so forth. Up-sampling by a factor k can be considered as\n",
      "being a convolution with a fractional input stride of 1/k. As a result, a\n",
      "convolution operator with input stride of 1/k can be applied backward (called\n",
      "deconvolution) with a stride of k [32].\n",
      "---\n",
      "4 Experimental Results\n",
      "---\n",
      "For implementing our spatio-temporal fully convolutional network (STFCN) we use\n",
      "the standard Caﬀe distribution [21] and a modiﬁed Caﬀe library with an LSTM\n",
      "implementation.1 We merged this LSTM implementations into the Caﬀe standard\n",
      "distribution and released our modiﬁed Caﬀe distribution to support new FCN\n",
      "layers that have been described in [32]. Our code has been tested on NVIDIA\n",
      "TITAN, and NVIDIA TITAN-X GPUs.2\n",
      "---\n",
      "To show the performance of our modiﬁed version of FCNs we use their im-\n",
      "plemented models for two cases, with and without our spatio-temporal module. We\n",
      "tested our STFCN networks on Camvid3 and NYUDv24 datasets. Our eval- uation\n",
      "methodology is as in other state-of-the-art semantic segmentation tests, such as\n",
      "in [1,32].\n",
      "---\n",
      "In the following, ﬁrst we describe the way how we embed our spatio-temporal\n",
      "module into FCNs and dilation convolution networks. Then we describe the metrics\n",
      "used in the evaluation process. After that we report our experiments on CamVid\n",
      "and NYUDv2. Finally, we discuss the performance of our method.\n",
      "---\n",
      "1 Available at github.com/junhyukoh/caffe-lstm 2 Our modiﬁed Caﬀe distribution\n",
      "and STFCN models are publicly available at https:\n",
      "---\n",
      "//github.com/MohsenFayyaz89/STFCN.\n",
      "---\n",
      "3 Available at mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/ 4 Available\n",
      "at cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "9\n",
      "---\n",
      "4.1 Embedding the Spatio-Temporal Module in FCN Networks\n",
      "---\n",
      "FCN-8 and FCN-32 [32] are fully convolutional versions of VGG-16 with some\n",
      "modiﬁcations to combine features of shallow layers with more precise spatial\n",
      "information with features of deeper layers which have more precise semantic\n",
      "information.\n",
      "---\n",
      "As mentioned in Section 3, it is of beneﬁt to embed the spatio-temporal module\n",
      "on top of the deepest layers. Thus we embed our spatio-temporal module on top of\n",
      "the f c7 layer of FCN-8 and FCN-32. The f c7 is the deepest fully convolutional\n",
      "layer which has large corresponding receptive ﬁelds in the input image. This\n",
      "layer extracts features which represent more semantic information in comparison\n",
      "to shallower layers.\n",
      "---\n",
      "An example of this modiﬁcation of an FCN-Alexnet is shown in Fig. 1. After\n",
      "embedding our spatio-temporal module in FCN-8 and FCN-32 networks, we call them\n",
      "STFCN-8 and STFCN-32. Our spatio-temporal module consists of LSTMs with 30\n",
      "hidden nodes and 3 time-steps for the CamVid dataset. We ﬁne-tuned our STFCN\n",
      "networks from pre-trained weights on PASCAL VOC [10] provided by [32]. We used a\n",
      "momentum amount of 0.9, and a learning rate of 10e-5.\n",
      "---\n",
      "4.2 Embedding Our Module in Dilated Convolution Networks\n",
      "---\n",
      "A dilated convolution network is an FCN network which beneﬁts from some\n",
      "modiﬁcations such as reducing down-sampling layers and using a context module\n",
      "which uses dilated convolutions. This module brings multi-scale ability to the\n",
      "network [51].\n",
      "---\n",
      "The dilated8 network [51] consists of two modules, front-end and context. The\n",
      "front-end module is based on a VGG-16 network with some modiﬁcations. The\n",
      "context layer is connected on top of this module. The f c7 layer of the front-\n",
      "end layer provides the main spatial features with 4,096 maps. This network has\n",
      "an input of size 900 × 1, 100. Because of removing some of its down-sampling\n",
      "layers, the f c7 layer has an output of size 66 × 91 which deﬁnes a high\n",
      "dimension for spatio-temporal computations. For overcoming this complexity\n",
      "problem, we down-sampled the output of this layer by a convolution layer to the\n",
      "size of 21×30, and fed it to our spatio-temporal module. Then, the spatio-\n",
      "temporal features are fed to a convolutional layer to decrease their maps to the\n",
      "size of the ﬁnal layer of the front-end module.\n",
      "---\n",
      "After resizing the maps, features are fed to a deconvolution layer to up-sample\n",
      "them to the size of the ﬁnal layer output (66 × 91). Finally, we fuse them with\n",
      "the front-end ﬁnal layer by an element-wise sum operation over all features.\n",
      "---\n",
      "The fused features are fed to the context module. Let STDilated8 be the modiﬁed\n",
      "version of dilated8; see Fig. 4. The spatio-temporal module of STDi- lated8\n",
      "consists of 30 hidden nodes of LSTMs with a time-step of 3. For training this\n",
      "network, we ﬁxed the front-end module and ﬁne-tuned the spatio-temporal and\n",
      "context modules with dilation8 pre-trained weights on CamVid. We used a momentum\n",
      "amount of 0.9, and a learning rate of 10e-5.\n",
      "---\n",
      "10\n",
      "---\n",
      "Authors Suppressed Due to Excessive Length\n",
      "---\n",
      "Fig. 4. Our STDilation8 model architecture\n",
      "---\n",
      "For better performance of the spatio-temporal module, we down-sampled the output\n",
      "of the f c7 layer of the dilation8 front-end module and fed it to the spatio-\n",
      "temporal module. Then we reduced the feature maps by a fully convolutional layer\n",
      "for a better description of the spatio-temporal features and make them the same\n",
      "size as the ﬁnal layer of the front-end module. Finally we up-sample and fuse\n",
      "the spatio-temporal features with the ﬁnal layer output and feed them into the\n",
      "context module.\n",
      "---\n",
      "4.3 Quality Measures for Evaluation\n",
      "---\n",
      "There are already various measures available for evaluating the accuracy of se-\n",
      "mantic segmentation. We describe most commonly used measures for accuracy\n",
      "evaluation which we have used to evaluate our method.\n",
      "---\n",
      "Mean intersection over union. Mean IU is a segmentation performance measure that\n",
      "quantiﬁes the overlap of two objects by calculating the ratio of the area of\n",
      "intersection to the area of unions [24,48]. This is a popular measure since it\n",
      "penalizes both over-segmentation and under-segmentation separately [38]. It is\n",
      "deﬁned as follows:\n",
      "---\n",
      "1ncl\n",
      "---\n",
      "(cid:88)\n",
      "---\n",
      "·\n",
      "---\n",
      "i\n",
      "---\n",
      "ti + (cid:80)\n",
      "---\n",
      "nii j nji − nii\n",
      "---\n",
      "(9)\n",
      "---\n",
      "where nii is the number of pixels of class i that is predicted correctly as\n",
      "belonging to class i, ti is the total number of pixels in class i, and ncl is\n",
      "the number of classes.\n",
      "---\n",
      "4.4 CamVid\n",
      "---\n",
      "The Cambridge-driving labelled video database (CamVid) [3] is a collection of\n",
      "videos with object-class semantic labels, complete with meta-data. The database\n",
      "provides ground truth labels that associate each pixel with one of 32 semantic\n",
      "classes. Like in [41], we partitioned the dataset into 367 training images, 100\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "11\n",
      "---\n",
      "Table 1. Evaluating FCNs and STFCNs for video semantic segmentation on Camvid\n",
      "(i.e. without or with our spatio-temporal module)\n",
      "---\n",
      "Mean IU\n",
      "---\n",
      "FCN-32s 46.1%\n",
      "---\n",
      "STFCN-32s\n",
      "---\n",
      "46.9%\n",
      "---\n",
      "FCN-8s 49.7%\n",
      "---\n",
      "STFCN-8s\n",
      "---\n",
      "50.6%\n",
      "---\n",
      "Table 2. Evaluating dilated convolution networks, without or with our module on\n",
      "Camvid\n",
      "---\n",
      "Mean IU\n",
      "---\n",
      "Dilation8 65.3%\n",
      "---\n",
      "STDilation8 (90 Frames)\n",
      "---\n",
      "65.9%\n",
      "---\n",
      "validation, and 233 test images. Eleven semantic classes are used in the\n",
      "selected images.\n",
      "---\n",
      "For FCN-8, FCN-32, STFCN-8, and STFCN-32, the images are down-sampled to 400 ×\n",
      "400. For dilation8 and STDilation8, the images are down-sampled to 640 × 480. As\n",
      "mentioned before, we used time-step 3 for our spatio-temporal module which means\n",
      "that we feed a sequence of 3 frames to our spatio-temporal networks.\n",
      "---\n",
      "The reason for choosing number 3 is that the annotated frames of CamVid have a\n",
      "distance of 30 frames to each other. In fact when we use 3 frames as a sequence,\n",
      "the ﬁrst and last frame of the sequence have a distance of 90 frames. Using more\n",
      "annotated frames is computationally possible because of the given LSTM\n",
      "abilities, but it is semantically wrong because of the high amount of changes in\n",
      "the frames.\n",
      "---\n",
      "Our results of FCNs and STFCNs tests on CamVid are shown in Table 1. It appears\n",
      "that adding our spatio-temporal module into FCN networks shows an improvement of\n",
      "their performance by close to one percent. Results for dilation8 and STDilation8\n",
      "tests on CamVid are shown in Table 2. The eﬀect of the spatio- temporal module\n",
      "is here an improvement by 0.8%. Improvements are in both cases not “dramatic”\n",
      "but consistent. Note that reports about improvements in the semantic\n",
      "segmentation area are typically in the sub-one-percent range [27,51,32].\n",
      "---\n",
      "Dilation8 achieves the best results in comparison to other work, and this is due\n",
      "to the power of multi-scale semantic segmentation. STDilation8 achieves even\n",
      "slightly better results because of beneﬁts from temporal features. Detailed\n",
      "results on the CamVid test set are reported in Table 3. Our model outperforms\n",
      "prior state-of-the-art work.\n",
      "---\n",
      "Table 3 shows that some approaches are competitive to related work such as Liu\n",
      "and He [29] with a performance superiority by 0.8 percent compared to SegNet\n",
      "[1]. In contrast, other approaches with a new base architecture achieved a\n",
      "better performance. Since our approach is based on FCN [32] or Dilation8 [51]\n",
      "methodologies, with our introduced spatio-temporal module, performance\n",
      "enhancement is close to one percent on FCN network, and close to 0.8 percent on\n",
      "Dilation8 architecture; both can be considered as a being a noticable en-\n",
      "hancement. Dilation + FSO [27] has been published recently based on Dilation8\n",
      "---\n",
      "12\n",
      "---\n",
      "Authors Suppressed Due to Excessive Length\n",
      "---\n",
      "Table 3. Our STDilation8 improves Dilation8 and outperforms prior work on Camvid\n",
      "---\n",
      "gnidliuB\n",
      "---\n",
      "eerT\n",
      "---\n",
      "ykS\n",
      "---\n",
      "raC\n",
      "---\n",
      "ngiS\n",
      "---\n",
      "daoR\n",
      "---\n",
      "nairtsedeP\n",
      "---\n",
      "ecneF\n",
      "---\n",
      "eloP\n",
      "---\n",
      "klawediS\n",
      "---\n",
      "tsilcycib\n",
      "---\n",
      "U\n",
      "---\n",
      "I\n",
      "---\n",
      "naem\n",
      "---\n",
      "73.4 70.2 91.1 64.2 24.4 91.1 29.1 31.0 13.6 72.4 28.6 53.6 ALE [28]\n",
      "SuperParsing [42] 70.4 54.8 83.5 43.3 25.4 83.4 11.6 18.3 5.2 57.4 8.90 42.0\n",
      "66.8 66.6 90.1 62.9 21.4 85.8 28.0 17.8 8.3 63.5 8.50 47.2 Liu and He [29]\n",
      "SegNet [1] 68.7 52.0 87.0 58.5 13.4 86.2 25.3 17.9 16.0 60.5 24.8 46.4 73.5 56.4\n",
      "90.7 63.3 17.9 90.1 31.4 21.7 18.2 64.9 29.3 50.6 STFCN-8 DeepLab-LFOV [7] 81.5\n",
      "74.6 89.0 82.2 42.3 92.2 48.4 27.2 14.3 75.4 50.1 61.6 Dilation8 [51] 82.6 76.2\n",
      "89.9 84.0 46.9 92.2 56.3 35.8 23.4 75.3 55.5 65.3 Dilation + FSO [27] 84.0 77.2\n",
      "91.3 85.6 49.9 92.5 59.1 37.6 16.9 76.0 57.2 66.1 83.4 76.5 90.4 84.6 50.4 92.4\n",
      "56.7 36.3 22.9 75.7 56.1 65.9 STDilation8\n",
      "---\n",
      "architecture and became state-of-the-art video semantic segmentation method. Our\n",
      "approach diﬀers from FSO in several ways:\n",
      "---\n",
      "– Our approach does not need any pre-processing or feature optimization for\n",
      "result enhancement. In contrast, FSO has used optical ﬂow as a feature set to be\n",
      "used by a CRF model. This is a computational operation which can be considered\n",
      "as a weakness for a semantic segmentation method. Computation eﬃciency and speed\n",
      "is very crucial in some tasks, such as, online video pro- cessing in advanced\n",
      "driver assistance systems. Some researches are ongoing to resolve optical ﬂow\n",
      "computational cost by using convolutional networks [11].\n",
      "---\n",
      "– We used time-step 3 for our spatio-temporal module to use the CamVid dataset\n",
      "annotations as-is without any preprocessing. This simplicity in de- sign and\n",
      "conﬁguration, is one of the strengths of our work.\n",
      "---\n",
      "– Our approach proposes an end-to-end network for semantic video segmenta-\n",
      "---\n",
      "tion which consists of spatial and temporal features altogether.\n",
      "---\n",
      "– In our approach we proposed a neural network based module for transforming\n",
      "traditional, fully convolutional networks into spatio-temporal CNNs. It can also\n",
      "be used for other related video processing tasks.\n",
      "---\n",
      "Also, we embedded our spatio-temporal module into FCN-Alexnet and evalu- ated\n",
      "its performance with and without our spatio-temporal module. Our spatio-\n",
      "temporal module improved its performance on CamVid dataset. Because the ba- sic\n",
      "FCN-Alexnet has a low performance for semantic segmentation as described in\n",
      "[32], so we decided not to include details into this paper.\n",
      "---\n",
      "4.5 NYUDv2\n",
      "---\n",
      "The NYU-Depth V2 data set is comprised of video sequences from a variety of\n",
      "indoor scenes recorded with an RGB-Depth camera [39]. It features 1,449 densely\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "13\n",
      "---\n",
      "Fig. 5. Outputs on CamVid. Top to bottom rows: Test samples, ground truth,\n",
      "Dilation8 [51], and STDilation8\n",
      "---\n",
      "labelled pairs of aligned RGB and depth images, including 464 new scenes taken\n",
      "at three cities, and 407,024 new unlabeled frames.\n",
      "---\n",
      "We selected this dataset to evaluate the eﬀect of multi-modal learning on our\n",
      "spatio-temporal module. Also, we tested our method on two totally diﬀerent\n",
      "datasets (outdoor vs. indoor) to evaluate its ﬂexibility. One problem of this\n",
      "dataset is that its annotated frames vary in length of sequences per subject or\n",
      "location. Thus, for this dataset, we do not use a constant time step for the\n",
      "spatio-temporal module. We fed sequences of diﬀerent lengths based on their\n",
      "location. This problem showed its eﬀect on results by decreasing the amount of\n",
      "improvements compared to none-temporal models.\n",
      "---\n",
      "Gupta et al. [16] coalesced NYU-Depth V2 into 40 classes. Similar to [32] we\n",
      "report results on a standard split into 795 training images and 654 test images.\n",
      "We selected our models based on [32] to be able to evaluate an embedding of our\n",
      "spatio-temporal module into their models. We use FCN-32s RGB and FCN-32s RGBD\n",
      "models to embed our spatio-temporal module in the way as explained before. Tests\n",
      "on NYUDv2 data are reported in Table 4.\n",
      "---\n",
      "Results show in this case the enhancement eﬀect of the spatio-temporal mod- ule\n",
      "on FCN-32s RGB and FCN-32s RGBD compared to the related networks FCN-32s RGB and\n",
      "FCN-32s RGBD, respectively.\n",
      "---\n",
      "14\n",
      "---\n",
      "Authors Suppressed Due to Excessive Length\n",
      "---\n",
      "Table 4. Evaluating STFCNs, FCNs, and dilated convolution networks for semantic\n",
      "video segmentation on NYUDv2\n",
      "---\n",
      "Network Gupta et al. [15] FCN-32s RGB [32] FCN-32s RGBD [32] STFCN-32s RGB\n",
      "STFCN-32s RGBD\n",
      "---\n",
      "Pixel Accuracy 60.3% 60.0% 61.5% 60.9% 62.1%\n",
      "---\n",
      "Mean Accuracy\n",
      "---\n",
      "− 42.2% 42.4% 42.3% 42.6%\n",
      "---\n",
      "Mean IU 28.6% 29.2% 30.5% 29.5% 30.9%\n",
      "---\n",
      "4.6 Discussion\n",
      "---\n",
      "We showed the power of our spatio-temporal module by embedding it into other\n",
      "known spatial, fully convolutional networks. In fact we introduced a spatio-\n",
      "temporal, fully convolutional network for extracting spatio-temporal features\n",
      "from video data and evaluated it based on two semantic segmentation case stud-\n",
      "ies.\n",
      "---\n",
      "Our module beneﬁts from the LSTM characteristics and is able to handle long-\n",
      "short term sequences. In our tests we were only able to use a limited number of\n",
      "video frames as being one sequence because of the limited number of available\n",
      "annotated frames. The method should also be tested on datasets with more\n",
      "extensive sets of annotated frames to check the eﬀect of sequence length on the\n",
      "performance of the system. It is possibly also of value to check the eﬀect of\n",
      "involving unannotated frames into input sequences by using prior or posterior\n",
      "annotated frames in the system.\n",
      "---\n",
      "5 Conclusions\n",
      "---\n",
      "In this paper we proposed a new architecture for spatio-temporal feature ex-\n",
      "traction from video. We designed and used this architecture for semantic video\n",
      "segmentation. First, a pre-trained CNN model was turned into an FCN model by\n",
      "changing classiﬁcation layers into fully convolutional layers. In this phase,\n",
      "spatial features from input frames can be used for classiﬁcation. But, in seman-\n",
      "tic video segmentation, relationships between frames can provide very useful\n",
      "information and enhance the accuracy of the segmentation program. Therefore,\n",
      "LSTM modules have been used to take advantage of temporal features. This\n",
      "architecture has been proposed as an end-to-end trainable model and can also be\n",
      "used for other vision tasks. Also, it does not need to be a pre-processing or\n",
      "post-processing module only, as we have seen in some other approaches.\n",
      "---\n",
      "We illustrated the performance of our architecture by embedding our spatio-\n",
      "temporal module into some state-of-the-art fully convolutional networks, such as\n",
      "FCN-VGG, and dilation convolution. Other types of LSTM modules have been\n",
      "proposed recently and have shown promising results for some vision tasks.\n",
      "Applying these newly proposed modules may enhance further the architecture of\n",
      "our spatio-temporal module, e.g. for scene understanding, anomaly detection in\n",
      "video, video captioning, object tracking, activity recognition, and so forth.\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "15\n",
      "---\n",
      "References\n",
      "---\n",
      "1. Badrinarayanan, V., Kendall, A., Cipolla, R.: SegNet: A deep convolu- tional\n",
      "encoder-decoder architecture for image segmentation. In: arXiv preprint\n",
      "arXiv:1511.00561, (2015).\n",
      "---\n",
      "2. Bittel, S., Kaiser, V., Teichmann, M., Thoma, M.: Pixel-wise segmentation of\n",
      "---\n",
      "street with neural networks. In: arXiv preprint arXiv:1511.00513, (2015)\n",
      "---\n",
      "3. Brostow, G.J., Shotton, J., Fauqueur, J., Cipolla, R.: Segmentation and\n",
      "recog- nition using structure from motion point clouds. In: ECCV, pp. 44–57\n",
      "(2008) 4. Carreira, J., Sminchisescu, C.: Constrained parametric min-cuts for\n",
      "automatic\n",
      "---\n",
      "object segmentation. In: CVPR, pp. 3241–3248 (2010)\n",
      "---\n",
      "5. Chang, F.J., Lin, Y.Y., Hsu, K.J.: Multiple structured-instance learning for\n",
      "semantic segmentation with uncertain training data. In: CVPR, pp. 360–367 (2014)\n",
      "---\n",
      "6. Chen, A.Y., Corso, J.J.: Propagating multi-class pixel labels throughout\n",
      "video\n",
      "---\n",
      "frames. In: Image Processing Workshop (WNYIPW), pp. 14–17 (2010)\n",
      "---\n",
      "7. Chen, L.C., Yang, Y., Wang, J., Xu, W., Yuille, A.L.: Attention to scale:\n",
      "Scale-aware semantic image segmentation. In: arXiv preprint arXiv:1511.03339\n",
      "(2015)\n",
      "---\n",
      "8. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Gated feedback recurrent neural\n",
      "---\n",
      "networks. In: arXiv preprint arXiv:1502.02367 (2015)\n",
      "---\n",
      "9. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan,\n",
      "S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for\n",
      "visual recognition and description. In: CVPR, pp. 2625-2634 (2015)\n",
      "---\n",
      "10. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The\n",
      "PASCAL visual object classes challenge 2011. In: www.pascal-network.org/\n",
      "challenges/VOC/voc2011/workshop/index.html, (2011)\n",
      "---\n",
      "11. Fischer, P., Dosovitskiy, A., Ilg, E., Husser, P., Hazrba, C., Golkov, V.,\n",
      "van der Smagt, P., Cremers, D. and Brox, T.: Flownet: Learning optical ﬂow with\n",
      "convolutional networks. In: arXiv preprint arXiv:1504.06852 (2015)\n",
      "---\n",
      "12. Galasso, F., Keuper, M., Brox, T., Schiele, B.: Spectral graph reduction for\n",
      "eﬃcient image and streaming video segmentation. In: CVPR, pp. 49–56 (2014) 13.\n",
      "Gers, F.A., Schmidhuber, J., Cummins, F.: Learning to forget: Continual pre-\n",
      "---\n",
      "diction with LSTM. In: Neural computation, pp. 2451–2471 (2000)\n",
      "---\n",
      "14. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies\n",
      "for accurate object detection and semantic segmentation. In: CVPR, pp. 580–587\n",
      "(2014)\n",
      "---\n",
      "15. Gupta, S., Arbelaez, P., Malik, J.: Perceptual organization and recognition\n",
      "of\n",
      "---\n",
      "indoor scenes from RGB-D images. In: CVPR, pp. 564–571 (2013)\n",
      "---\n",
      "16. Gupta, S., Girshick, R., Arbel´aez, P., Malik, J.: Learning rich features\n",
      "from RGB-D images for object detection and segmentation. In: ECCV, pp. 345–360\n",
      "(2014)\n",
      "---\n",
      "17. He, Y., Chiu, W.C., Keuper, M., Fritz, M.: RGBD semantic segmentation us-\n",
      "ing spatio-temporal data-driven pooling. In: arXiv preprint arXiv:1604.02388\n",
      "(2016)\n",
      "---\n",
      "18. Hickson, S., Birchﬁeld, S., Essa, I., Christensen, H.: Eﬃcient hierarchical\n",
      "graph-\n",
      "---\n",
      "based segmentation of RGBD videos. In: CVPR, pp. 344–351 (2014)\n",
      "---\n",
      "19. Hochreiter, S., Schmidhuber, J.: Long short-term memory. In: Neural compu-\n",
      "---\n",
      "tation, pp. 1735–1780 (1997)\n",
      "---\n",
      "16\n",
      "---\n",
      "Authors Suppressed Due to Excessive Length\n",
      "---\n",
      "20. Hong, S., Noh, H., Han, B.: Decoupled deep neural network for semi-\n",
      "supervised\n",
      "---\n",
      "semantic segmentation. In: NIPS, pp. 1495–1503 (2015)\n",
      "---\n",
      "21. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,\n",
      "Guadarrama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature\n",
      "embedding. In: Proc. ACM Int. Conf. Multimedia, pp. 675–678 (2014)\n",
      "---\n",
      "22. Kalchbrenner, N., Danihelka, I., Graves, A.: Grid long short-term memory.\n",
      "In:\n",
      "---\n",
      "arXiv preprint arXiv:1507.01526 (2015)\n",
      "---\n",
      "23. Khoreva, A., Galasso, F., Hein, M., Schiele, B.: Classiﬁer based graph\n",
      "construc-\n",
      "---\n",
      "tion for video segmentation. In: CVPR, pp. 951–960 (2015)\n",
      "---\n",
      "24. Klette, R., Rosenfeld, A.: Digital Geometry. Morgan Kaufmann, San Francisco\n",
      "---\n",
      "(2004)\n",
      "---\n",
      "25. Koutnik, J., Greﬀ, K., Gomez, F., Schmidhuber, J.: A clockwork RNN. In:\n",
      "---\n",
      "arXiv preprint arXiv:1402.3511 (2014)\n",
      "---\n",
      "26. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with\n",
      "deep convolutional neural networks. In: Advances Neural Information Processing\n",
      "Systems, pp. 1097–1105 (2012)\n",
      "---\n",
      "27. Kundu, A., Vineet, V., Koltun, V.: Feature space optimization for semantic\n",
      "---\n",
      "video segmentation. In: CVPR, (2016)\n",
      "---\n",
      "28. Russell, C., Kohli, P., Torr, P.H.: Associative hierarchical CRFs for object\n",
      "class\n",
      "---\n",
      "image segmentation. In: ICCV, pp. 739–746 (2009)\n",
      "---\n",
      "29. Liu, B., He, X.: Multiclass semantic video segmentation with object-level\n",
      "active\n",
      "---\n",
      "inference. In: CVPR, pp. 4286–4294 (2015)\n",
      "---\n",
      "30. Liu, X., Tao, D., Song, M., Ruan, Y., Chen, C., Bu, J.: Weakly supervised\n",
      "---\n",
      "multiclass video segmentation. In: CVPR, pp. 57–64 (2014)\n",
      "---\n",
      "31. Liu, Y., Liu, J., Li, Z., Tang, J., Lu, H.: Weakly-supervised dual\n",
      "clustering for\n",
      "---\n",
      "image semantic segmentation. In: CVPR, pp. 2075–2082 (2013)\n",
      "---\n",
      "32. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for\n",
      "semantic\n",
      "---\n",
      "segmentation. In: CVPR, pp. 3431–3440 (2015)\n",
      "---\n",
      "33. Martinovic, A., Knopp, J., Riemenschneider, H., Van Gool, L.: 3D all the\n",
      "way: Semantic segmentation of urban scenes from start to end in 3D. In: CVPR,\n",
      "pp. 4456–4465 (2015)\n",
      "---\n",
      "34. Matan, O., Burges, C.J., LeCun, Y., Denker, J.S.: Multi-digit recognition\n",
      "using\n",
      "---\n",
      "a space displacement neural network. In: NIPS, pp. 488–495 (1991)\n",
      "---\n",
      "35. Mottaghi, R., Fidler, S., Yao, J., Urtasun, R., Parikh, D.: Analyzing\n",
      "semantic segmentation using hybrid human-machine CRFs. In: CVPR, pp. 3143–3150\n",
      "(2013)\n",
      "---\n",
      "36. Richmond, D.L., Kainmueller, D., Yang, M.Y., Myers, E.W., Rother, C.: Relat-\n",
      "ing cascaded random forests to deep convolutional neural networks for semantic\n",
      "segmentation. In: arXiv preprint arXiv:1507.07583 (2015)\n",
      "---\n",
      "37. Sabokrou, M., Fathy, M., Hoseini, M., Klette, R.: Real-time anomaly\n",
      "detection and localization in crowded scenes. In: CVPR, workshops, pp. 56–62\n",
      "(2015) 38. Sharma, A., Tuzel, O., Jacobs, D.W.: Deep hierarchical parsing for\n",
      "semantic\n",
      "---\n",
      "segmentation. In: CVPR, pp. 530–538 (2015)\n",
      "---\n",
      "39. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and\n",
      "sup-\n",
      "---\n",
      "port inference from RGBD images. In: ECCV, pp. 746–760 (2012)\n",
      "---\n",
      "40. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action\n",
      "recognition in videos. In: Advances Neural Information Processing Systems, pp.\n",
      "68–576 (2014)\n",
      "---\n",
      "41. Sturgess, P., Alahari, K., Ladicky, L., Torr, P.H.: Combining appearance and\n",
      "structure from motion features for road scene understanding. In: BMVC, (2012)\n",
      "---\n",
      "STFCN: Spatio-Temporal FCN for Semantic Video Segmentation\n",
      "---\n",
      "17\n",
      "---\n",
      "42. Tighe, J., Lazebnik, S.: Superparsing: Scalable nonparametric image parsing\n",
      "---\n",
      "with superpixels. In: ECCV, pp. 352–365 (2010)\n",
      "---\n",
      "43. Volpi, M., Ferrari, V.: Semantic segmentation of urban scenes by learning\n",
      "local\n",
      "---\n",
      "class interactions, In: CVPR, pp. 1–9 (2015)\n",
      "---\n",
      "44. Wolf, R., Platt, J.C.: Postal address block location using a convolutional\n",
      "locator\n",
      "---\n",
      "network. In: NIPS pp. 745–745 (1994)\n",
      "---\n",
      "45. Yang, Y., Hallman, S., Ramanan, D., Fowlkes, C.C.: Layered object models for\n",
      "---\n",
      "image segmentation. IEEE Trans. PAMI, pp. 1731–1743 (2012)\n",
      "---\n",
      "46. Zheng, C., Wang, L.: Semantic segmentation of remote sensing imagery using\n",
      "object-based Markov random ﬁeld model with regional penalties. IEEE J. Se-\n",
      "lected Topics in Applied Earth Observations Remote Sensing, pp. 1924–1935 (2015)\n",
      "---\n",
      "47. Zhang, L., Song, M., Liu, Z., Liu, X., Bu, J., Chen, C.: Probabilistic\n",
      "graphlet cut: Exploiting spatial structure cue for weakly supervised image\n",
      "segmentation. In: CVPR, pp. 1908–1915 (2013)\n",
      "---\n",
      "48. Zhang, Y., Chen, X., Li, J., Wang, C., Xia, C.: Semantic object segmentation\n",
      "---\n",
      "via detection in weakly labeled video. In: CVPR, pp. 3641–3649 (2015)\n",
      "---\n",
      "49. Zhu, Y., Urtasun, R., Salakhutdinov, R., Fidler, S.: Segdeepm: Exploiting\n",
      "seg- mentation and context in deep neural networks for object detection. In:\n",
      "CVPR, pp. 4703–4711 (2015)\n",
      "---\n",
      "50. Yao, K., Cohn, T., Vylomova, K., Duh, K., Dyer, C.: Depth-gated LSTM. In:\n",
      "---\n",
      "arXiv preprint arXiv:1508.03790 (2015)\n",
      "---\n",
      "51. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions.\n",
      "In:\n",
      "---\n",
      "ICLR (2016)\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "result = extract_word_bounding_boxes(\"test.pdf\")\n",
    "page = result[2]\n",
    "words = page['word_bboxes']\n",
    "# result = \" | \".join([w[\"word\"] for w in words])\n",
    "# print(textwrap.fill(result, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
